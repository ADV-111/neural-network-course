{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krakowiakpawel9/neural-network-course/blob/master/07_rnn/01_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_X7jpgqlRyb",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing danych tekstowch - wektoryzacja tekstu\n",
        "\n",
        "1. [Podział tekstu na słowa](#a0)\n",
        "2. [Kodowanie *one_hot()*](#a1)\n",
        "3. [Kodowanie *hashing_trick()*](#a2)\n",
        "4. [Tokenizer](#a3)\n",
        "\n",
        "Nie możemy wprowadzić bezpośrednio danych tekstowych do sieci neuronowej! Musimy te dane odpowiednio przygotować (preprocessing). Dane tekstowe muszą zostać zakodowane za pomocą liczb aby mogły być wprowadzone do sieci neuronowej. \n",
        "\n",
        "Biblioteka Keras zawiera kilka narzędzi, które możemy wykorzystać do przygotowania naszych danych. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-78Qb1vslXCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vlI9iE3pLNa",
        "colab_type": "text"
      },
      "source": [
        "### <a name='a0'></a> Podział tekstu na słowa\n",
        "\n",
        "Często w NLP - Natural Language Processing używamy słowa token. Tokenem może być pojedyńczy znak, a także cały wyraz. Wsystko zależy od kontekstu i potrzeb. Apy podzielić tekst na tokeny (w tym przypadku wyrazy) użyjemy funkcji *text_to_word_sequence()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me1IMXXHpIgh",
        "colab_type": "code",
        "outputId": "7f20dda6-414e-4df2-b5a3-192d5519c88e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = 'Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.'\n",
        "\n",
        "tokens = text_to_word_sequence(text)\n",
        "tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keras',\n",
              " 'is',\n",
              " 'a',\n",
              " 'high',\n",
              " 'level',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'api',\n",
              " 'written',\n",
              " 'in',\n",
              " 'python',\n",
              " 'and',\n",
              " 'capable',\n",
              " 'of',\n",
              " 'running',\n",
              " 'on',\n",
              " 'top',\n",
              " 'of',\n",
              " 'tensorflow',\n",
              " 'cntk',\n",
              " 'or',\n",
              " 'theano']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b73WDqyIqchw",
        "colab_type": "text"
      },
      "source": [
        "### <a name='a1'></a> Kodowanie *one_hot()*\n",
        "\n",
        "Polega na przedstawieniu każdego słowa jako unikalnej liczby całkowitej. *one_hot(text, n)* koduje tekst do listy indeksów słów o rozmiarze n. Jest to opakowanie funkcji hashing_trick używającej hash jako funkcji hashującej. Jednoznaczność mapowania słów na indeksy nie jest gwarantowana. Nazwa sugeruje, że tworzymy kodowanie zero-jednykowe dokumentu, co nie jest prawdą. Zamiast tego funkcja jest opakowaniem dla funkcji hashing_trick () opisanej w następnej sekcji. Funkcja zwraca wersję dokumentu zakodowaną liczbami całkowitymi. Zastosowanie funkcji hashującej może powodować kolizje i nie wszystkim słowom zostaną przypisane unikalne wartości całkowite.\n",
        "\n",
        "Oprócz tekstu należy podać rozmiar słownika. Może to być łączna liczba słów w dokumencie lub więcej, jeśli zamierzamy zakodować dodatkowe dokumenty zawierające dodatkowe słowa. Rozmiar słownika określa przestrzeń hashująca, z której słowa są hashowane. Najlepiej byłoby, gdyby był on większy niż słownik o pewien procent (np. 25%), aby zminimalizować liczbę kolizji. Domyślnie używana jest funkcja hash."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BVXixRQGH8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "139ffb70-c056-4a5e-ed40-76952e32f453"
      },
      "source": [
        "hash('sieć')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4081765782712657403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWeGs1X3Gf4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "27d5bd2a-96b5-4be5-8990-c71a8b6b491f"
      },
      "source": [
        "hash('sieć') % 100"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61BcpUUiGPXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "15bdf3d5-199a-4756-b7f2-34dc16088279"
      },
      "source": [
        "hash('neuronowa')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-6029288004248942092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TBwBWt4GTJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e186dcf-f3d1-44a7-b506-4653c5cbce2c"
      },
      "source": [
        "hash('sieć')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4081765782712657403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd_I9uaprTty",
        "colab_type": "code",
        "outputId": "369d8867-2e96-4f6e-941f-85b27734d623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "words = set(tokens)\n",
        "one_hot_tokens = one_hot(text, round(len(words) * 1.3))\n",
        "one_hot_tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13,\n",
              " 9,\n",
              " 18,\n",
              " 22,\n",
              " 17,\n",
              " 11,\n",
              " 18,\n",
              " 21,\n",
              " 21,\n",
              " 26,\n",
              " 4,\n",
              " 12,\n",
              " 7,\n",
              " 18,\n",
              " 20,\n",
              " 5,\n",
              " 19,\n",
              " 18,\n",
              " 18,\n",
              " 19,\n",
              " 6,\n",
              " 18]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Hqu8FJuzeS",
        "colab_type": "text"
      },
      "source": [
        "### <a name='a2'></a> Kodowanie *hashing_trick()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5E8P4w2u3Zv",
        "colab_type": "code",
        "outputId": "ffd19713-be88-4805-a8f2-af780606a617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "\n",
        "hashing_trick(text, round(len(words) * 1.3), hash_function='md5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7,\n",
              " 20,\n",
              " 22,\n",
              " 24,\n",
              " 20,\n",
              " 14,\n",
              " 19,\n",
              " 11,\n",
              " 10,\n",
              " 4,\n",
              " 4,\n",
              " 15,\n",
              " 19,\n",
              " 15,\n",
              " 18,\n",
              " 23,\n",
              " 14,\n",
              " 15,\n",
              " 5,\n",
              " 20,\n",
              " 2,\n",
              " 8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfMd5M9TlwPy",
        "colab_type": "text"
      },
      "source": [
        "### <a name='a3'></a> Tokenizer\n",
        "Keras dostarcza klasę Tokenizer do przygotowywania dokumentów tekstowych do uczenia głębokiego. \n",
        "\n",
        "Klasa *Tokenizer* pozwala zamienić każdy tekst na sekwencję liczb całkowitych w taki sposób, że każda liczba całkowita jest indeksem tokenu w słowniku. Tokenem zazwyczaj jest pojedyncze słowo. Zero jest zarezerwowanym ideksem i nie może zostać przypisane do żadnego słowa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4HmQB8elcfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# num_words - zachowana zostanie określona liczba słów ze względu na częstotliwość\n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9RmalTmmrxp",
        "colab_type": "code",
        "outputId": "86a4aced-de0d-4b0f-ea77-55fde4a07252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "samples = ['Great picture!', 'Nice view', 'Good to see you :)', 'Good picture!', 'Good']\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "tokenizer.index_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'good',\n",
              " 2: 'picture',\n",
              " 3: 'great',\n",
              " 4: 'nice',\n",
              " 5: 'view',\n",
              " 6: 'to',\n",
              " 7: 'see',\n",
              " 8: 'you'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juXUFP0nHJ5",
        "colab_type": "code",
        "outputId": "ba486fe7-57f7-4138-abf9-5e90cb54c96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "tokenizer.word_counts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('great', 1),\n",
              "             ('picture', 2),\n",
              "             ('nice', 1),\n",
              "             ('view', 1),\n",
              "             ('good', 3),\n",
              "             ('to', 1),\n",
              "             ('see', 1),\n",
              "             ('you', 1)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-DWVcdeoBj2",
        "colab_type": "code",
        "outputId": "a0147aff-31f3-4128-c51c-0eaf613ad689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.document_count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n0LPVKZwNxo",
        "colab_type": "text"
      },
      "source": [
        "Po dopasowaniu Tokenizera do danych treningowych można go użyć do kodowania dokumentów danych treningowych jak i danych testowych.\n",
        "\n",
        "Funkcja *texts_to_matrix()* tworzy wektor dla każdego dokumentu. Długość wektora jest równa długości unikalnych słów we wszystkich dokumentach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcQY6xIVyNSu",
        "colab_type": "code",
        "outputId": "08e0c9f6-bb56-4556-d1bc-7d93afdeb3be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tokenizer.index_word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'good', 2: 'picture', 3: 'great', 4: 'nice', 5: 'view', 6: 'to', 7: 'see', 8: 'you'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoHbCSCvoQu0",
        "colab_type": "code",
        "outputId": "85c9f870-d5ac-4569-a0e9-f8f0f4db3d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "tokenizer.texts_to_matrix(samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    }
  ]
}